---
id: Research
aliases: []
tags:
  - llm
draft: false
title: Research
---
https://cloud.google.com/vertex-ai/generative-ai/docs/learn-resources

Using Ray with Vertex AI
https://medium.com/@RTae/leveraging-ray-and-vertex-ai-for-llmops-6b169f65ff3a

Ray and LLM finetuning
https://www.anyscale.com/blog/announcing-ray-2-4-0-infrastructure-for-llm-training-tuning-inference-and

AirLLM and Llama 70B in 4GB GPU!
https://medium.com/ai-advances/run-the-strongest-open-source-llm-model-llama3-70b-with-just-a-single-4gb-gpu-7e0ea2ad8ba2

Ray + DeepSpeed 
https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms

GKE LLM
https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-multiple-gpu

Japanese article talking about Tokenizer
https://tech.mntsq.co.jp/entry/2020/12/16/160006
https://tech.mntsq.co.jp/entry/2021/02/26/120013

# Use this Japanese Wikipedia base
Where do they get the Japanese wiki data?
https://yag-ays.github.io/project/cirrus/

I found the one that is used in the above article
https://huggingface.co/tohoku-nlp/bert-base-japanese

It seem like the model is generated using below code
Source code: https://github.com/cl-tohoku/bert-japanese/tree/v2.0
Im analyzing it here [[bert-japanese]]


